{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM AUTOENCODER",
   "id": "ab7d2f664fbd97f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:14.142485Z",
     "start_time": "2024-07-20T02:09:14.137877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parts of this code are inspired by \n",
    "# GeeksforGeeks (2022): Implementing an Autoencoder in PyTorch. Available online at https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/, updated on 7/7/2022, checked on 7/16/2024."
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.456861Z",
     "start_time": "2024-07-20T02:09:14.144025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.cycling_dataset import TorchCyclingDataset\n",
    "from notebooks.autoencoder.lstm_ae import LSTM_AE\n",
    "\n",
    "from tqdm import tqdm\n",
    "from statistics import mean"
   ],
   "id": "95ff0b14496232a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\mwenn\\AppData\\Local\\Temp\\ipykernel_7404\\2080661305.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "C:\\Users\\mwenn\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.461964Z",
     "start_time": "2024-07-20T02:09:16.457890Z"
    }
   },
   "cell_type": "code",
   "source": "BATCH_SIZE = 8",
   "id": "f3bcfe2a6f7df81a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot(losses):\n",
    "    # Defining the Plot Style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "     \n",
    "    # Plotting\n",
    "    plt.plot(losses)"
   ],
   "id": "fe9f436b96b73807",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.570749Z",
     "start_time": "2024-07-20T02:09:16.473305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load preprocessed datasets\n",
    "backwheel_test = TorchCyclingDataset(file_path_acc=\"../../data/preprocessed/backwheel_acc_test.h5\",\n",
    "                             file_path_gyro=\"../../data/preprocessed/backwheel_gyro_test.h5\")\n",
    "backwheel_train = TorchCyclingDataset(file_path_acc=\"../../data/preprocessed/backwheel_acc_train.h5\",\n",
    "                              file_path_gyro=\"../../data/preprocessed/backwheel_gyro_train.h5\")\n",
    "\n",
    "handlebar_test = TorchCyclingDataset(file_path_acc=\"../../data/preprocessed/handlebar_acc_test.h5\",\n",
    "                             file_path_gyro=\"../../data/preprocessed/handlebar_gyro_test.h5\")\n",
    "handlebar_train = TorchCyclingDataset(file_path_acc=\"../../data/preprocessed/handlebar_acc_train.h5\",\n",
    "                              file_path_gyro=\"../../data/preprocessed/handlebar_gyro_train.h5\")"
   ],
   "id": "7d5e332b614ffba",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.577943Z",
     "start_time": "2024-07-20T02:09:16.571766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataLoader is used to load the dataset \n",
    "# for training\n",
    "backwheel_test_loader = torch.utils.data.DataLoader(dataset = backwheel_test,\n",
    "                                     batch_size = BATCH_SIZE,\n",
    "                                     shuffle = True)\n",
    "backwheel_train_loader = torch.utils.data.DataLoader(dataset = backwheel_train,\n",
    "                                     batch_size = BATCH_SIZE,\n",
    "                                     shuffle = True)\n",
    "\n",
    "handlebar_test_loader = torch.utils.data.DataLoader(dataset = handlebar_test,\n",
    "                                     batch_size = BATCH_SIZE,\n",
    "                                     shuffle = True)\n",
    "handlebar_train_loader = torch.utils.data.DataLoader(dataset = handlebar_train,\n",
    "                                     batch_size = BATCH_SIZE,\n",
    "                                     shuffle = True)"
   ],
   "id": "c401e708525870e2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.592429Z",
     "start_time": "2024-07-20T02:09:16.579481Z"
    }
   },
   "cell_type": "code",
   "source": "next(iter(handlebar_train_loader))[0].shape",
   "id": "c01aa5c7fa7b7442",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.601111Z",
     "start_time": "2024-07-20T02:09:16.593961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from https://github.com/shobrook/sequitur/blob/master/sequitur/quick_train.py, 2024-07-20\n",
    "\n",
    "def train_model(model, train_set, verbose, lr, epochs):\n",
    "    # model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    mean_losses = []\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "        for batch in train_set:\n",
    "            for x in batch:\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # Forward pass\n",
    "                x_prime = model(x)\n",
    "    \n",
    "                loss = criterion(x_prime, x)\n",
    "    \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                losses.append(loss.item())\n",
    "\n",
    "        mean_loss = mean(losses)\n",
    "        mean_losses.append(mean_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch}, Loss: {mean_loss}\")\n",
    "\n",
    "    return mean_losses"
   ],
   "id": "17512a0dbdd01d62",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.609965Z",
     "start_time": "2024-07-20T02:09:16.602646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Autoencoder model\n",
    "model = LSTM_AE(input_dim=6,\n",
    "                  encoding_dim=2,\n",
    "                  h_dims=[5,3],\n",
    "                  h_activ=torch.nn.ReLU(),\n",
    "                  out_activ=torch.nn.Sigmoid(),\n",
    "                  )"
   ],
   "id": "7574bd17107323a0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.616615Z",
     "start_time": "2024-07-20T02:09:16.611006Z"
    }
   },
   "cell_type": "code",
   "source": "model.encoder",
   "id": "28e4bfe1d5f71b90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): LSTM(6, 5, batch_first=True)\n",
       "    (1): LSTM(5, 3, batch_first=True)\n",
       "    (2): LSTM(3, 2, batch_first=True)\n",
       "  )\n",
       "  (h_activ): ReLU()\n",
       "  (out_activ): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:09:16.623324Z",
     "start_time": "2024-07-20T02:09:16.617640Z"
    }
   },
   "cell_type": "code",
   "source": "model.decoder",
   "id": "6ace02fd2e8c031e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): LSTM(2, 3, batch_first=True)\n",
       "    (1): LSTM(3, 5, batch_first=True)\n",
       "    (2): LSTM(5, 5, batch_first=True)\n",
       "  )\n",
       "  (h_activ): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:11:51.785515Z",
     "start_time": "2024-07-20T02:09:16.624872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "mean_losses = train_model(model, handlebar_train_loader, verbose=False, lr=1e-3, epochs=epochs)"
   ],
   "id": "b85ade9884f6210f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mean_losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandlebar_train_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 23\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_set, verbose, lr, epochs)\u001B[0m\n\u001B[0;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(x_prime, x)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     26\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    521\u001B[0m     )\n\u001B[1;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ComTec-I\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T02:11:51.787036Z",
     "start_time": "2024-07-20T02:11:51.786025Z"
    }
   },
   "cell_type": "code",
   "source": "plot(mean_losses)",
   "id": "a1973fdf0f92fab8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), f'./saved_models/lstm_encoder-{epochs}_epochs.pt')",
   "id": "378c2166aa2ed326",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
